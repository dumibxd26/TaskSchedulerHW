# Opinion

Working on the suite changed my intuition about what actually limits scheduler performance. Before the experiments I imagined the policies themselves would dominate: FIFO would be simple but unfair, priority would keep latency low for critical jobs, and round-robin would trade optimality for fairness. After instrumenting the services, it became obvious that the control plane is the first thing to saturate. Every `next` and `done` call is a REST hop through FastAPI; with 16 workers × 16 cores the scheduler spends more time answering HTTP than arranging jobs. The lesson is that theoretical policy comparisons are moot if the plumbing cannot keep pace.

Priority scheduling was the most surprising. I expected classic starvation for low-priority work, and we did see multi-second tail latencies for background batches. But the instrumentation also showed that high-priority bursts can overload L3 caches and elevate CPU usage enough to slow *all* jobs, not just the low-priority ones. In other words, favoring important work can still backfire if the implementation does not throttle or smooth arrivals. The fix was not algorithmic but practical: we added heap-based ready queues plus arrival threads to keep load admission smooth.

Round-robin delivered the most educational fairness story. In datasets with small quanta (1–5 ms simulated), the number of slices per job skyrocketed, which means the number of scheduler round-trips grew linearly with (jobs × slices). That is precisely the distributed version of context-switch overhead. Watching `avg_slices_per_job` climb above 400 made it tangible that fairness costs real bandwidth. I now think of fairness as a budgeted resource: every extra slice should be justified by a metric such as slowdown-based Jain’s index, not just by tradition.

FIFO, while seemingly naïve, became the baseline that kept us honest. Its single-pass execution allowed us to isolate how much of the latency inflation in other policies came from coordination rather than workload variability. In the burst datasets it produced predictable queue buildup, and because workers run jobs to completion, the amount of scheduler chatter was minimal. That provided a clean control condition for debugging the more complex paths.

Another takeaway concerns automation hygiene. `run_matrix.py` originally supported a small subset of configurations, but extending it to the full Cartesian grid unlocked a more scientific workflow: edit code, rerun the grid, compare `summary.csv` snapshots. The script also taught me to treat infrastructure (starting uvicorn servers, waiting for readiness, copying CSVs) as code; copy-pasting shell commands was too brittle for 16×16 experiments. The carefully named directories—`results/replicas_X_cores_Y`—now serve as a public data package satisfying the coursework requirement for dataset sharing.

Finally, the project clarified the value of instrumentation. Prometheus endpoints and structured JSON logs let us correlate scheduler CPU usage with queue sizes and job completion rates. Without those signals the qualitative observations above would have remained anecdotes. The experience cemented the idea that “opinion” pieces about system behavior must be grounded in reproducible evidence, not just intuition.
